{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fpdf import FPDF\n",
    "from PyPDF2 import PdfFileMerger,PdfFileReader\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Pipeline:\n",
    "    def __init__(self, default_model=\"llama2\", doc=\"output.pdf\"):\n",
    "        self.model_name = default_model\n",
    "        self.doc_path = doc\n",
    "        self.model = Ollama(model=self.model_name)\n",
    "        self.embeddings = OllamaEmbeddings()\n",
    "        self.parser = StrOutputParser()\n",
    "        self.loader = PyPDFLoader(self.doc_path)\n",
    "        self.pages = self.loader.load_and_split()\n",
    "        self.vectorstore = DocArrayInMemorySearch.from_documents(self.pages, embedding=self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        self.default_prompt_template = \"\"\"\n",
    "        Answer the question based on the context below. If you cannot answer the question, reply \"I don't know\".\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        self.default_prompt = PromptTemplate.from_template(self.default_prompt_template)\n",
    "\n",
    "        self.default_chain = (\n",
    "            {\n",
    "                \"context\": itemgetter(\"question\") | self.retriever, \n",
    "                \"question\": itemgetter(\"question\")\n",
    "            }\n",
    "            | self.default_prompt\n",
    "            | self.model\n",
    "            | self.parser\n",
    "        )\n",
    "\n",
    "    def fetch(self, question, prompt_type=\"default\"):\n",
    "        if prompt_type == \"explicit\":prompt = self.get_explicit_prompt(question)\n",
    "        elif prompt_type == \"role\":prompt = self.get_role_prompt(question)\n",
    "        elif prompt_type == \"chain_of_thought\":prompt = self.get_chain_of_thought_prompt(question)\n",
    "        elif prompt_type == \"self_consistency\":return self.get_self_consistent_response(question)\n",
    "        else:prompt = self.default_prompt\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": itemgetter(\"question\") | self.retriever, \n",
    "                \"question\": itemgetter(\"question\")\n",
    "            }\n",
    "            | prompt\n",
    "            | self.model\n",
    "            | self.parser\n",
    "        )\n",
    "        return chain.invoke({'question': question})\n",
    "\n",
    "    def get_explicit_prompt(self,question):\n",
    "        template = \"\"\"\n",
    "        You are an AI model that provides detailed and specific answers. Answer the question based on the context below. If you cannot answer the question, reply \"I don't know\". Limit your response to 250 words.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        return PromptTemplate.from_template(template)\n",
    "\n",
    "    def get_role_prompt(self,question):\n",
    "        template = \"\"\"\n",
    "        You are a project manager with extensive experience in assigning tasks based on individual strengths. Answer the question based on the context below. If you cannot answer the question, reply \"I don't know\".\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: Who is the best suited for the project {question}, say the name and explain why that person is best suited for this project\n",
    "        \"\"\"\n",
    "        return PromptTemplate.from_template(template)\n",
    "\n",
    "    def get_chain_of_thought_prompt(self,question):\n",
    "        template = \"\"\"\n",
    "        Let's think through this carefully, step by step. Answer the question based on the context below. If you cannot answer the question, reply \"I don't know\".\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        return PromptTemplate.from_template(template)\n",
    "\n",
    "    def get_self_consistent_response(self,question):\n",
    "        explanations = []\n",
    "        for _ in range(5):\n",
    "            chain = (\n",
    "                {\n",
    "                    \"context\": itemgetter(\"question\") | self.retriever, \n",
    "                    \"question\": itemgetter(\"question\")\n",
    "                }\n",
    "                | self.default_prompt | self.model | self.parser\n",
    "            )\n",
    "            explanations.append(chain.invoke({'question': question}))\n",
    "        most_consistent=max(set(explanations),key=explanations.count)\n",
    "        return most_consistent\n",
    "\n",
    "    def get_metadata_for_chunk(self, chunk_id):\n",
    "        metadata=self.vectorstore.get_metadata(chunk_id)\n",
    "        return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RAG_Pipeline()\n",
    "user_input=\"Develop a comprehensive proposal for a Retrieval-Augmented Generation (RAG) project utilizing ChromaDB and Ollama, detailing the integration of advanced retrieval mechanisms with generative models, data pipelines, evaluation metrics, project timeline, and anticipated performance improvements.\"\n",
    "answer=model.fetch(user_input)\n",
    "print(\"\\n\" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RAG_Pipeline()\n",
    "user_input=\"explain what is cnn?\"\n",
    "prompt_types=[\"default\", \"explicit\", \"role\", \"chain_of_thought\", \"self_consistency\"]\n",
    "answers ={}\n",
    "\n",
    "for prompt_type in prompt_types:\n",
    "    timestamp=datetime.now().isoformat()\n",
    "    answer=model.fetch(user_input, prompt_type=prompt_type)\n",
    "    answers[prompt_type]={\"timestamp\":timestamp,\"answer\":answer}\n",
    "\n",
    "with open(\"answers.json\",\"w\") as file:json.dump(answers,file,indent=4)\n",
    "\n",
    "\n",
    "for prompt_type, content in answers.items():\n",
    "    print(f\"\\n{prompt_type} prompt:\\n{content['answer']}\\n\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(answer,correct_answer):\n",
    "    reference=[correct_answer.split()]\n",
    "    candidate=answer.split()\n",
    "    return sentence_bleu(reference,candidate)\n",
    "def compute_rouge(answer,correct_answer):\n",
    "    scorer=rouge_scorer.RougeScorer(['rouge1','rougeL'],use_stemmer=True)\n",
    "    scores=scorer.score(correct_answer,answer)\n",
    "    return scores\n",
    "\n",
    "def compute_bertscore(answer,correct_answer):\n",
    "    _, _,F1=bert_score([answer],[correct_answer],lang='en')\n",
    "    return F1.mean().item()\n",
    "\n",
    "with open(\"answers.json\",\"r\") as file:answers =json.load(file)\n",
    "\n",
    "correct_answer = \"\"\"A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data like images. It consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply filters to the input data to extract features, pooling layers reduce the dimensionality, and fully connected layers perform the final classification or regression tasks. CNNs are widely used in image recognition, object detection, and other computer vision tasks due to their ability to automatically learn spatial hierarchies of features from input data.\"\"\"\n",
    "\n",
    "\n",
    "similarities = {}\n",
    "for prompt_type,content in answers.items():\n",
    "    answer=content[\"answer\"]\n",
    "    bleu_score=compute_bleu(answer, correct_answer)\n",
    "    rouge_scores=compute_rouge(answer, correct_answer)\n",
    "    bertscore=compute_bertscore(answer, correct_answer)\n",
    "    similarities[prompt_type]={\"BLEU\":bleu_score,\"ROUGE-1\":rouge_scores['rouge1'].fmeasure,\"ROUGE-L\":rouge_scores['rougeL'].fmeasure,\"BERTScore\":bertscore}\n",
    "labels=list(similarities.keys())\n",
    "bleu_scores=[similarities[label][\"BLEU\"] for label in labels]\n",
    "rouge1_scores=[similarities[label][\"ROUGE-1\"] for label in labels]\n",
    "rougeL_scores=[similarities[label][\"ROUGE-L\"] for label in labels]\n",
    "bertscores=[similarities[label][\"BERTScore\"] for label in labels]\n",
    "\n",
    "x =range(len(labels))\n",
    "\n",
    "fig,axs =plt.subplots(2,2,figsize=(14,10))\n",
    "axs[0,0].bar(x,bleu_scores,width=0.4,label='BLEU',color='blue',align='center')\n",
    "axs[0,0].set_xlabel('Prompt Type')\n",
    "axs[0,0].set_ylabel('Score')\n",
    "axs[0,0].set_title('BLEU Score')\n",
    "axs[0,0].set_xticks(x)\n",
    "axs[0,0].set_xticklabels(labels)\n",
    "axs[0,0].set_ylim([0,1])\n",
    "axs[0,1].bar(x, rouge1_scores,width=0.4,label='ROUGE-1', color='orange', align='center')\n",
    "axs[0,1].set_xlabel('Prompt Type')\n",
    "axs[0,1].set_ylabel('Score')\n",
    "axs[0,1].set_title('ROUGE-1 Score')\n",
    "axs[0,1].set_xticks(x)\n",
    "axs[0,1].set_xticklabels(labels)\n",
    "axs[0,1].set_ylim([0, 1])\n",
    "axs[1,0].bar(x,rougeL_scores,width=0.4,label='ROUGE-L',color='green',align='center')\n",
    "axs[1,0].set_xlabel('Prompt Type')\n",
    "axs[1,0].set_ylabel('Score')\n",
    "axs[1,0].set_title('ROUGE-L Score')\n",
    "axs[1,0].set_xticks(x)\n",
    "axs[1,0].set_xticklabels(labels)\n",
    "axs[1,0].set_ylim([0, 1])\n",
    "axs[1,1].bar(x,bertscores,width=0.4,label='BERTScore', color='red',align='center')\n",
    "axs[1,1].set_xlabel('Prompt Type')\n",
    "axs[1,1].set_ylabel('Score')\n",
    "axs[1,1].set_title('BERTScore')\n",
    "axs[1,1].set_xticks(x)\n",
    "axs[1,1].set_xticklabels(labels)\n",
    "axs[1,1].set_ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for prompt_type, scores in similarities.items():\n",
    "    print(f\"{prompt_type} prompt:\")\n",
    "    print(f\" BLEU: {scores['BLEU']:.4f}\")\n",
    "    print(f\" ROUGE-1: {scores['ROUGE-1']:.4f}\")\n",
    "    print(f\" ROUGE-L: {scores['ROUGE-L']:.4f}\")\n",
    "    print(f\" BERTScore: {scores['BERTScore']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.json\",\"r\") as file:answers=json.load(file)\n",
    "similarities={}\n",
    "for prompt_type, content in answers.items():\n",
    "    answer=content[\"answer\"]\n",
    "    bleu_score=compute_bleu(answer, correct_answer)\n",
    "    rouge_scores =compute_rouge(answer, correct_answer)\n",
    "    bertscore =compute_bertscore(answer, correct_answer)\n",
    "    start_time = datetime(2024,6,23, 13, 15, 36)\n",
    "    end_time = datetime.fromisoformat(content[\"timestamp\"])\n",
    "    time_taken = (end_time - start_time).total_seconds()\n",
    "    similarities[prompt_type] = {\"BLEU\": bleu_score,\"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\"BERTScore\": bertscore,\"Time\": time_taken}\n",
    "\n",
    "metrics = [\"BLEU\", \"ROUGE-1\", \"ROUGE-L\", \"BERTScore\"]\n",
    "\n",
    "max_values={metric:max([similarities[prompt][metric] for prompt in similarities]) for metric in metrics}\n",
    "max_time = max([similarities[prompt][\"Time\"] for prompt in similarities])\n",
    "normalized_scores={}\n",
    "for prompt in similarities:\n",
    "    normalized_scores[prompt]={}\n",
    "    for metric in metrics:\n",
    "        normalized_scores[prompt][metric]=similarities[prompt][metric]/max_values[metric]\n",
    "    normalized_scores[prompt][\"Time\"]=similarities[prompt][\"Time\"]/max_time\n",
    "\n",
    "\n",
    "w1 = 0.7\n",
    "w2 = 0.3\n",
    "\n",
    "final_scores={}\n",
    "for prompt in normalized_scores:\n",
    "    q=np.mean([normalized_scores[prompt][metric] for metric in metrics])\n",
    "    te=1-normalized_scores[prompt][\"Time\"]\n",
    "    final_scores[prompt]=w1*q+w2*te\n",
    "\n",
    "ordered_prompts = sorted(final_scores.keys(), key=lambda x: final_scores[x], reverse=True)\n",
    "print(\"Final Scores:\")\n",
    "for prompt in ordered_prompts:print(f\"{prompt} prompt: {final_scores[prompt]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(ordered_prompts, [final_scores[prompt] for prompt in ordered_prompts], color='skyblue')\n",
    "plt.xlabel('Prompt Type')\n",
    "plt.ylabel('Final Score')\n",
    "plt.title('Final Scores for Prompt Types Considering Quality and Time Efficiency')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "for prompt_type, scores in similarities.items():\n",
    "    print(f\"{prompt_type} prompt:\")\n",
    "    print(f\" BLEU: {scores['BLEU']:.4f}\")\n",
    "    print(f\" ROUGE-1: {scores['ROUGE-1']:.4f}\")\n",
    "    print(f\" ROUGE-L: {scores['ROUGE-L']:.4f}\")\n",
    "    print(f\" BERTScore: {scores['BERTScore']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
