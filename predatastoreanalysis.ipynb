{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79A9RCvW3QNq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from gensim.models import Word2Vec\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sScQiQdY3QNr",
        "outputId": "76a18636-f7ed-4e61-93f3-4b5968735522"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0dfhhKt3QNs"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text=text.lower()\n",
        "    text=re.sub(r'<[^>]+>', '', text) \n",
        "    text=re.sub(r'http[s]?://\\S+', '', text)\n",
        "    text=text.replace('\\n', ' ').replace('\\r', '').replace('\\t',' ')\n",
        "    text=re.sub(r'\\s+', ' ', text)\n",
        "    text=text.translate(str.maketrans('','', string.punctuation))\n",
        "    tokens=word_tokenize(text)\n",
        "    stop_words=set(stopwords.words('english'))\n",
        "    tokens=[word for word in tokens if word not in stop_words]\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-jBed9l3QNs"
      },
      "outputs": [],
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self,doc_path:str):\n",
        "        self.doc_path=doc_path\n",
        "        self.loader=PyPDFLoader(self.doc_path)\n",
        "    def process_document(self):\n",
        "        document_pages=self.loader.load_and_split()\n",
        "        raw_pages=[doc.page_content for doc in document_pages]\n",
        "        full_text= ' '.join(raw_pages)\n",
        "        cleaned_text=clean_text(full_text)\n",
        "        return full_text, cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8dJEsdb39VE"
      },
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(texts_before,texts_after):\n",
        "    vectorizer=TfidfVectorizer()\n",
        "    tfidf_before=vectorizer.fit_transform(texts_before)\n",
        "    tfidf_after=vectorizer.transform(texts_after)\n",
        "    similarity_scores =cosine_similarity(tfidf_before, tfidf_after)\n",
        "    return np.diag(similarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRolKmvH3_QP"
      },
      "outputs": [],
      "source": [
        "def generate_word_clouds(text_before,text_after):\n",
        "    wc_before=WordCloud(background_color='white').generate(text_before)\n",
        "    wc_after=WordCloud(background_color='white').generate(text_after)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(wc_before,interpolation='bilinear')\n",
        "    plt.title('before Cleaning')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(wc_after,interpolation='bilinear')\n",
        "    plt.title('after cleaning')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnxvg9wF4Cf8"
      },
      "outputs": [],
      "source": [
        "def evaluate_topics(texts,num_topics=5,passes=20, chunksize=50):\n",
        "    tokenized_texts=[word_tokenize(text.lower()) for text in texts]\n",
        "    dictionary=Dictionary(tokenized_texts)\n",
        "    corpus =[dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "    lda =LdaModel(corpus,num_topics=num_topics,id2word=dictionary,passes=passes,chunksize=chunksize,random_state=42,update_every=1)\n",
        "    coherence_model =CoherenceModel(model=lda,texts=tokenized_texts,dictionary=dictionary,coherence='c_v')\n",
        "    coherence= coherence_model.get_coherence()\n",
        "    return coherence,lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h0iyOjH6gsM"
      },
      "outputs": [],
      "source": [
        "def plot_term_frequency(text,title=\"term frequency\"):\n",
        "    words=text.split()\n",
        "    frequencies=Counter(words)\n",
        "    labels,values=zip(*frequencies.most_common(30))\n",
        "    indexes=range(len(labels))\n",
        "    width=1\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(indexes,values,width)\n",
        "    plt.xticks(indexes,labels,rotation='vertical')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_UkFNyS6b3S"
      },
      "outputs": [],
      "source": [
        "def plot_ngram_frequency(text,n=2,num_ngrams=20,title=\"N-gram frequency\"):\n",
        "    words=word_tokenize(text)\n",
        "    n_grams=ngrams(words, n)\n",
        "    frequencies =Counter(n_grams)\n",
        "    labels,values =zip(*frequencies.most_common(num_ngrams))\n",
        "    labels=[' '.join(gram) for gram in labels]\n",
        "    indexes=range(len(labels))\n",
        "    width=1\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.bar(indexes,values,width)\n",
        "    plt.xticks(indexes,labels,rotation='vertical')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHk6b-Z06nd-"
      },
      "outputs": [],
      "source": [
        "def visualize_embeddings(text, title=\"word embedding visualization\",annotate_top_n=20):\n",
        "    words=[word_tokenize(text)]\n",
        "    model=Word2Vec(words, min_count=1, vector_size=50, window=3, workers=4)\n",
        "    labels=[]\n",
        "    tokens =[]\n",
        "    word_frequencies =[(word,model.wv.get_vecattr(word,\"count\")) for word in model.wv.key_to_index]\n",
        "    word_frequencies.sort(key=lambda x:x[1],reverse=True)\n",
        "    for word,_ in word_frequencies:\n",
        "        tokens.append(model.wv[word])\n",
        "        labels.append(word)\n",
        "    tokens_np =np.array(tokens)\n",
        "    tsne_model=TSNE(perplexity=5,n_components=2, init='pca',n_iter=2500,random_state=23)\n",
        "    new_values=tsne_model.fit_transform(tokens_np)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i, value in enumerate(new_values):\n",
        "        plt.scatter(value[0],value[1])\n",
        "        if i < annotate_top_n:plt.annotate(labels[i],xy=(value[0],value[1]),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4_ZWmfR44Mm",
        "outputId": "744fbaa2-9b60-4b82-da84-dd526e050763"
      },
      "outputs": [],
      "source": [
        "processor=DocumentProcessor(doc_path=\"doc.pdf\")\n",
        "text_before_cleaning,text_after_cleaning= processor.process_document()\n",
        "similarity_scores = compute_cosine_similarity([text_before_cleaning],[text_after_cleaning])\n",
        "print(\"cosine similarity scores:\",similarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "QWl7SiiB3QNt",
        "outputId": "851de1d6-6a9f-48db-f849-b4591ff46bf5"
      },
      "outputs": [],
      "source": [
        "generate_word_clouds(text_before_cleaning,text_after_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qq6IoT5ssD",
        "outputId": "b7d93209-24e7-472c-9ea7-53fa566d1ce0"
      },
      "outputs": [],
      "source": [
        "coherence_before, lda_before =evaluate_topics([text_before_cleaning], passes=20, chunksize=50)\n",
        "coherence_after, lda_after = evaluate_topics([text_after_cleaning], passes=20, chunksize=50)\n",
        "print(f'coherence - before cleaning: {coherence_before}')\n",
        "print(f'coherence - after cleaning: {coherence_after}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gFftEqSF5vM1",
        "outputId": "040d64b2-34e0-4e08-d45a-169cdeb0b563"
      },
      "outputs": [],
      "source": [
        "plot_term_frequency(text_before_cleaning,\"term frequency before cleaning\")\n",
        "plot_term_frequency(text_after_cleaning,\"term frequency after cleaning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zuLWHOkp6qM6",
        "outputId": "8c6cd59b-1051-43e6-cf8b-185b476b3fb5"
      },
      "outputs": [],
      "source": [
        "plot_ngram_frequency(text_before_cleaning,n=2,title=\"bi-gram frequency before cleaning\")\n",
        "plot_ngram_frequency(text_after_cleaning,n=2,title=\"bi-gram frequency after cleaning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ezvgL0fn60U7",
        "outputId": "fbd1555c-6d28-410c-b31f-8bc9700391e3"
      },
      "outputs": [],
      "source": [
        "visualize_embeddings(text_before_cleaning,\"embedding space before cleaning\")\n",
        "visualize_embeddings(text_after_cleaning,\"embedding space after cleaning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
